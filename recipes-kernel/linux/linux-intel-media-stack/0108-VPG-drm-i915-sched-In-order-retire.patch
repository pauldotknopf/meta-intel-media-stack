From 3ab39ebf3f3a45bd746d9fe7bbc01d278542dd5c Mon Sep 17 00:00:00 2001
From: John Harrison <John.C.Harrison@Intel.com>
Date: Wed, 9 Mar 2016 09:05:56 +0000
Subject: [PATCH 108/153] [VPG]: drm/i915/sched: In order retire

Retire the request in order
---
 drivers/gpu/drm/i915/i915_gem.c |   38 ++++++++++++++++++--------------------
 1 files changed, 18 insertions(+), 20 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index dbdee03..a6ed001 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -3438,12 +3438,6 @@ i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
 	 */
 	i915_gem_request_notify(ring, false);
 
-	/*
-	 * Note that request entries might be out of order due to rescheduling
-	 * and pre-emption. Thus both lists must be processed in their entirety
-	 * rather than stopping at the first non-complete entry.
-	 */
-
 	/* Retire requests first as we use it above for the early return.
 	 * If we retire requests last, we may use a later seqno and so clear
 	 * the requests lists without clearing the active list, leading to
@@ -3451,7 +3445,7 @@ i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
 	 */
 	list_for_each_entry_safe(req, req_next, &ring->request_list, list) {
 		if (!i915_gem_request_completed(req))
-			continue;
+			break;
 
 		i915_gem_request_retire(req);
 	}
@@ -3462,7 +3456,7 @@ i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
 	 */
 	list_for_each_entry_safe(obj, obj_next, &ring->active_list, ring_list[ring->id]) {
 		if (!list_empty(&obj->last_read_req[ring->id]->list))
-			continue;
+			break;
 
 		i915_gem_object_retire__read(obj, ring->id);
 	}
@@ -3474,21 +3468,25 @@ i915_gem_retire_requests_ring(struct intel_engine_cs *ring)
 	}
 
 	/* Tidy up any requests that were recently signalled */
-	spin_lock_irq(&ring->fence_lock);
-	list_splice_init(&ring->fence_unsignal_list, &list_head);
-	spin_unlock_irq(&ring->fence_lock);
-	list_for_each_entry_safe(req, req_next, &list_head, unsignal_link) {
-		list_del(&req->unsignal_link);
-		i915_gem_request_unreference(req);
+	if (!list_empty(&ring->fence_unsignal_list)) {
+		spin_lock_irq(&ring->fence_lock);
+		list_splice_init(&ring->fence_unsignal_list, &list_head);
+		spin_unlock_irq(&ring->fence_lock);
+		list_for_each_entry_safe(req, req_next, &list_head, unsignal_link) {
+			list_del(&req->unsignal_link);
+			i915_gem_request_unreference(req);
+		}
 	}
 
 	/* Really free any requests that were recently unreferenced */
-	spin_lock(&ring->delayed_free_lock);
-	list_splice_init(&ring->delayed_free_list, &list_head);
-	spin_unlock(&ring->delayed_free_lock);
-	list_for_each_entry_safe(req, req_next, &list_head, delayed_free_link) {
-		list_del(&req->delayed_free_link);
-		i915_gem_request_free(req);
+	if (!list_empty(&ring->delayed_free_list)) {
+		spin_lock(&ring->delayed_free_lock);
+		list_splice_init(&ring->delayed_free_list, &list_head);
+		spin_unlock(&ring->delayed_free_lock);
+		list_for_each_entry_safe(req, req_next, &list_head, delayed_free_link) {
+			list_del(&req->delayed_free_link);
+			i915_gem_request_free(req);
+		}
 	}
 
 	WARN_ON(i915_verify_lists(ring->dev));
-- 
1.7.1

