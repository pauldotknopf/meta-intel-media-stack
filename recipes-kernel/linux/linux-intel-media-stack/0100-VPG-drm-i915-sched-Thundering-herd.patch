From e50d72795d37dd555c70bd654328fa8e47ff6874 Mon Sep 17 00:00:00 2001
From: John Harrison <John.C.Harrison@Intel.com>
Date: Wed, 16 Dec 2015 12:43:48 +0000
Subject: [PATCH 100/153] [VPG]: drm/i915/sched: Thundering herd

Reduce the thread number to be waked up when one request is
completed.
---
 drivers/gpu/drm/i915/i915_debugfs.c     |    3 +-
 drivers/gpu/drm/i915/i915_drv.h         |    3 ++
 drivers/gpu/drm/i915/i915_gem.c         |   56 ++++++++++++++++++++++++++----
 drivers/gpu/drm/i915/i915_gpu_error.c   |    2 +-
 drivers/gpu/drm/i915/i915_irq.c         |   22 ++++++++----
 drivers/gpu/drm/i915/intel_lrc.c        |    3 +-
 drivers/gpu/drm/i915/intel_ringbuffer.c |    4 +-
 drivers/gpu/drm/i915/intel_ringbuffer.h |    3 +-
 8 files changed, 74 insertions(+), 22 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index e1ad113..40cf87c 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -710,11 +710,12 @@ static int i915_gem_request_info(struct seq_file *m, void *data)
 			task = NULL;
 			if (req->pid)
 				task = pid_task(req->pid, PIDTYPE_PID);
-			seq_printf(m, "    %x @ %d: %s [%d], fence = %x:%x\n",
+			seq_printf(m, "    %x @ %d: %s [%d], wait = %d, fence = %x:%x\n",
 				   req->seqno,
 				   (int) (jiffies - req->emitted_jiffies),
 				   task ? task->comm : "<unknown>",
 				   task ? task->pid : -1,
+				   req->wait_count,
 				   req->fence.context, req->fence.seqno);
 			rcu_read_unlock();
 		}
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index ca9038f..b396858 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -2276,9 +2276,12 @@ struct drm_i915_gem_request {
 	struct list_head signal_link;
 	struct list_head unsignal_link;
 	struct list_head delayed_free_link;
+	struct list_head wait_link;
+	uint32_t wait_count;
 	bool cancelled;
 	bool irq_enabled;
 	bool signal_requested;
+	wait_queue_head_t locked_wait_queue;
 
 	/** On Which ring this request was generated */
 	struct drm_i915_private *i915;
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index e755ce0..9a38aed 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -1257,8 +1257,8 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 	struct drm_i915_private *dev_priv = dev->dev_private;
 	int state = interruptible ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE;
 	uint32_t seqno;
-	DEFINE_WAIT(wait);
-	unsigned long timeout_expire;
+	DEFINE_WAIT(locked_wait);
+	unsigned long timeout_expire, flags;
 	s64 before = 0; /* Only to silence a compiler warning. */
 	int ret = 0;
 	bool busy;
@@ -1302,13 +1302,26 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 	 */
 	fence_enable_sw_signaling(&req->fence);
 
+	spin_lock_irqsave(&ring->request_wait_lock, flags);
+	req->wait_count++;
+	if (req->wait_count == 1)
+		list_add_tail(&req->wait_link, &ring->request_wait_list);
+	spin_unlock_irqrestore(&ring->request_wait_lock, flags);
+
 	for (;;) {
 		struct timer_list timer;
+		signed long result, fence_timeout;
 
-		prepare_to_wait(&ring->irq_queue, &wait, state);
+		if (is_locked) 
+			prepare_to_wait(&req->locked_wait_queue, &locked_wait, state);
 
+		// ???
+		// Not required because in the case of a GPU reset, the request's fence will
+		// get signalled (but in an errored state) and the waiter will be woken up.
+		// But need to release the mutex lock in order to get to the errored fence!?
+		// ???
 		/* We need to check whether any gpu reset happened in between
-		 * the caller grabbing the seqno and now ... */
+		 * the caller grabbing the request and now ... */
 		if (reset_counter != atomic_read(&dev_priv->gpu_error.reset_counter)) {
 			/* ... but upgrade the -EAGAIN to an -EIO if the gpu
 			 * is truely gone. */
@@ -1375,7 +1388,19 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 			mod_timer(&timer, expire);
 		}
 
-		io_schedule();
+		if (is_locked) {
+			io_schedule();
+		} else if (timeout) {
+			fence_timeout = nsecs_to_jiffies_timeout((u64) *timeout);
+			if ((fence_timeout > MAX_SCHEDULE_TIMEOUT) || (fence_timeout < 0)) {
+				printk(KERN_ERR "%s:%d \x1B[35;1mInvalid timeout: %ld <- 0x%08X:%08X!\x1B[0m\n", __func__, __LINE__,
+				       fence_timeout, (uint32_t) ((*timeout) >> 32), (uint32_t) ((*timeout) & 0xFFFFFFFF));
+				fence_timeout = 1000;
+			}
+			result = fence_wait_timeout(&req->fence, interruptible, fence_timeout);
+		} else {
+			result = fence_wait(&req->fence, interruptible);
+		}
 
 		if (timer.function) {
 			del_singleshot_timer_sync(&timer);
@@ -1383,7 +1408,18 @@ int __i915_wait_request(struct drm_i915_gem_request *req,
 		}
 	}
 
-	finish_wait(&ring->irq_queue, &wait);
+	/* Check for aborted execution, e.g due to GPU reset: */
+	if (req->fence.status < 0)
+		ret = -EIO;
+
+	if (is_locked)
+		finish_wait(&req->locked_wait_queue, &locked_wait);
+
+	spin_lock_irqsave(&ring->request_wait_lock, flags);
+	req->wait_count--;
+	if (req->wait_count == 0)
+		list_del(&req->wait_link);
+	spin_unlock_irqrestore(&ring->request_wait_lock, flags);
 
 out:
 	trace_i915_gem_request_wait_end(req);
@@ -1501,6 +1537,7 @@ static void i915_gem_request_retire(struct drm_i915_gem_request *request)
 		/* How to propagate to any associated sync_fence??? */
 		request->fence.status = -EIO;
 		fence_signal_locked(&request->fence);
+		wake_up_all(&request->locked_wait_queue);
 	}
 
 	if (request->scheduler_qe)
@@ -2963,6 +3000,7 @@ void i915_gem_request_notify(struct intel_engine_cs *ring, bool fence_locked)
 		if (!req->cancelled) {
 			fence_signal_locked(&req->fence);
 			trace_i915_gem_request_complete(req);
+			wake_up_all(&req->locked_wait_queue);
 		}
 
 		if (req->irq_enabled) {
@@ -2977,8 +3015,7 @@ void i915_gem_request_notify(struct intel_engine_cs *ring, bool fence_locked)
 	if (!fence_locked)
 		spin_unlock_irqrestore(&ring->fence_lock, flags);
 
-	/* Necessary? Or does the fence_signal() call do an implicit wakeup? */
-	wake_up_all(&ring->irq_queue);
+//	wake_up_all(&ring->locked_wait_queue);
 
 	/* Final scheduler processing after all individual updates are done. */
 	if (wake_sched)
@@ -3198,6 +3235,7 @@ int i915_gem_request_alloc(struct intel_engine_cs *ring,
 		goto err;
 	}
 
+	init_waitqueue_head(&req->locked_wait_queue);
 	INIT_LIST_HEAD(&req->ctx_link);
 	INIT_LIST_HEAD(&req->signal_link);
 	fence_init(&req->fence, &i915_gem_request_fops, &ring->fence_lock,
@@ -3242,6 +3280,8 @@ void i915_gem_request_cancel(struct drm_i915_gem_request *req)
 	req->fence.status = -EINVAL;
 	fence_signal_locked(&req->fence);
 
+	wake_up_all(&req->locked_wait_queue);
+
 	i915_gem_request_unreference(req);
 }
 
diff --git a/drivers/gpu/drm/i915/i915_gpu_error.c b/drivers/gpu/drm/i915/i915_gpu_error.c
index 2f04e4f..305b096 100644
--- a/drivers/gpu/drm/i915/i915_gpu_error.c
+++ b/drivers/gpu/drm/i915/i915_gpu_error.c
@@ -889,7 +889,7 @@ static void i915_record_ring_state(struct drm_device *dev,
 		ering->instdone = I915_READ(GEN2_INSTDONE);
 	}
 
-	ering->waiting = waitqueue_active(&ring->irq_queue);
+	ering->waiting = !list_empty(&ring->request_wait_list);
 	ering->instpm = I915_READ(RING_INSTPM(ring->mmio_base));
 	ering->seqno = ring->get_seqno(ring, false);
 	ering->acthd = intel_ring_get_active_head(ring);
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index 8d357af..592e5c1 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -982,8 +982,6 @@ static void notify_ring(struct intel_engine_cs *ring)
 		return;
 
 	i915_gem_request_notify(ring, false);
-
-	wake_up_all(&ring->irq_queue);
 }
 
 static void vlv_c0_read(struct drm_i915_private *dev_priv,
@@ -2378,9 +2376,13 @@ static void i915_error_wake_up(struct drm_i915_private *dev_priv,
 	 * a gpu reset pending so that i915_error_work_func can acquire them).
 	 */
 
-	/* Wake up __wait_seqno, potentially holding dev->struct_mutex. */
-	for_each_ring(ring, dev_priv, i)
-		wake_up_all(&ring->irq_queue);
+	/* Wake up __wait_seqno which are holding dev->struct_mutex. */
+	for_each_ring(ring, dev_priv, i) {
+		struct drm_i915_gem_request *req;
+
+		list_for_each_entry(req, &ring->request_wait_list, wait_link)
+			wake_up_all(&req->locked_wait_queue);
+	}
 
 	/* Wake up intel_crtc_wait_for_pending_flips, holding crtc->mutex. */
 	wake_up_all(&dev_priv->pending_flip_queue);
@@ -3023,8 +3025,10 @@ static void i915_hangcheck_elapsed(struct work_struct *work)
 			if (ring_idle(ring, seqno)) {
 				ring->hangcheck.action = HANGCHECK_IDLE;
 
-				if (waitqueue_active(&ring->irq_queue)) {
-					/* Issue a wake-up to catch stuck h/w. */
+				if (!list_empty(&ring->request_wait_list)) {
+					printk(KERN_ERR "<%s> \x1B[35mwait list active\x1B[0m\n", ring->name);
+					/* Do an explicit seqno check to catch stuck h/w. */
+//					/* Issue a wake-up to catch stuck h/w. */
 					if (!test_and_set_bit(ring->id, &dev_priv->gpu_error.missed_irq_rings)) {
 						if (!(dev_priv->gpu_error.test_irq_rings & intel_ring_flag(ring)))
 							DRM_ERROR("Hangcheck timer elapsed... %s idle\n",
@@ -3032,7 +3036,9 @@ static void i915_hangcheck_elapsed(struct work_struct *work)
 						else
 							DRM_INFO("Fake missed irq on %s\n",
 								 ring->name);
-						wake_up_all(&ring->irq_queue);
+						printk(KERN_ERR "<%s> \x1B[35mRing hung!\x1B[0m\n", ring->name);
+						i915_gem_request_notify(ring, false);
+						//wake_up_all(&ring->irq_queue);
 					}
 					/* Safeguard against driver failure */
 					ring->hangcheck.score += BUSY;
diff --git a/drivers/gpu/drm/i915/intel_lrc.c b/drivers/gpu/drm/i915/intel_lrc.c
index 7c57844..f8a2931 100644
--- a/drivers/gpu/drm/i915/intel_lrc.c
+++ b/drivers/gpu/drm/i915/intel_lrc.c
@@ -2101,13 +2101,14 @@ static int logical_ring_init(struct drm_device *dev, struct intel_engine_cs *rin
 	ring->dev = dev;
 	INIT_LIST_HEAD(&ring->active_list);
 	INIT_LIST_HEAD(&ring->request_list);
+	INIT_LIST_HEAD(&ring->request_wait_list);
 	INIT_LIST_HEAD(&ring->fence_signal_list);
 	INIT_LIST_HEAD(&ring->fence_unsignal_list);
 	INIT_LIST_HEAD(&ring->delayed_free_list);
 	spin_lock_init(&ring->fence_lock);
 	spin_lock_init(&ring->delayed_free_lock);
+	spin_lock_init(&ring->request_wait_lock);
 	i915_gem_batch_pool_init(dev, &ring->batch_pool);
-	init_waitqueue_head(&ring->irq_queue);
 
 	INIT_LIST_HEAD(&ring->buffers);
 	INIT_LIST_HEAD(&ring->execlist_queue);
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.c b/drivers/gpu/drm/i915/intel_ringbuffer.c
index 5506de0..3b1ffae 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.c
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.c
@@ -2132,6 +2132,7 @@ static int intel_init_ring_buffer(struct drm_device *dev,
 	ring->dev = dev;
 	INIT_LIST_HEAD(&ring->active_list);
 	INIT_LIST_HEAD(&ring->request_list);
+	INIT_LIST_HEAD(&ring->request_wait_list);
 	INIT_LIST_HEAD(&ring->execlist_queue);
 	INIT_LIST_HEAD(&ring->buffers);
 	INIT_LIST_HEAD(&ring->fence_signal_list);
@@ -2139,11 +2140,10 @@ static int intel_init_ring_buffer(struct drm_device *dev,
 	INIT_LIST_HEAD(&ring->delayed_free_list);
 	spin_lock_init(&ring->fence_lock);
 	spin_lock_init(&ring->delayed_free_lock);
+	spin_lock_init(&ring->request_wait_lock);
 	i915_gem_batch_pool_init(dev, &ring->batch_pool);
 	memset(ring->semaphore.sync_seqno, 0, sizeof(ring->semaphore.sync_seqno));
 
-	init_waitqueue_head(&ring->irq_queue);
-
 	ringbuf = intel_engine_create_ringbuffer(ring, 32 * PAGE_SIZE);
 	if (IS_ERR(ringbuf))
 		return PTR_ERR(ringbuf);
diff --git a/drivers/gpu/drm/i915/intel_ringbuffer.h b/drivers/gpu/drm/i915/intel_ringbuffer.h
index eb41461..43b262c 100644
--- a/drivers/gpu/drm/i915/intel_ringbuffer.h
+++ b/drivers/gpu/drm/i915/intel_ringbuffer.h
@@ -313,7 +313,8 @@ struct  intel_engine_cs {
 
 	bool gpu_caches_dirty;
 
-	wait_queue_head_t irq_queue;
+	spinlock_t request_wait_lock;
+	struct list_head request_wait_list;
 
 	struct intel_context *default_context;
 	struct intel_context *last_context;
-- 
1.7.1

