From 219d18c40e0cfebad4dec930e16c41f718800d9e Mon Sep 17 00:00:00 2001
From: John Harrison <John.C.Harrison@Intel.com>
Date: Wed, 10 Jun 2015 15:42:34 +0100
Subject: [PATCH 056/153] drm/i915: Add sync wait support to scheduler

There is a sync framework to allow work for multiple independent
systems to be synchronised with each other but without stalling
the CPU whether in the application or the driver. This patch adds
support for this framework to the GPU scheduler.

Batch buffers can now have sync framework fence objects associated with
them. The scheduler will look at this fence when deciding what to
submit next to the hardware. If the fence is outstanding then that
batch buffer will be passed over in preference of one that is ready to
run. If no other batches are ready then the scheduler will queue an
asynchronous callback to be woken up when the fence has been
signalled. The callback will wake the scheduler and submit the now
ready batch buffer.

v0.2: Updated to use the new sync_fence_is_signaled() API rather than
having to know about the internals of a fence object.

Also removed the spin lock from the fence wait callback function. This
was used to clear the 'waiting' flag. However, it causes a problem
with the TDR code. Specifically, when cancelling work packets due to a
TDR there is a code path where the fence can be signalled while the
spinlock is already held. It is not really necessary to clear the flag
anyway as it's purpose is solely to prevent multiple waits being
issued. Once the fence has been signalled, no further waits will be
attempted so it doesn't matter whether the fence is marked as having
an outstanding wait or not.

v0.3: Re-wrapped long lines and comments to keep style checker happy.

*v?5?* Collapsed clean up code to a single instance.

For: VIZ-1587
Signed-off-by: John Harrison <John.C.Harrison@Intel.com>
---
 drivers/gpu/drm/i915/i915_drv.h       |    1 +
 drivers/gpu/drm/i915/i915_scheduler.c |  144 +++++++++++++++++++++++++++++++-
 drivers/gpu/drm/i915/i915_scheduler.h |    8 ++
 3 files changed, 148 insertions(+), 5 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 8da1826..602e8ae 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -1738,6 +1738,7 @@ struct i915_execbuffer_params {
 	uint64_t                        batch_obj_vm_offset;
 	struct intel_engine_cs          *ring;
 	struct drm_i915_gem_object      *batch_obj;
+	struct sync_fence               *fence_wait;
 	struct drm_clip_rect            *cliprects;
 	uint32_t                        instp_mask;
 	int                             instp_mode;
diff --git a/drivers/gpu/drm/i915/i915_scheduler.c b/drivers/gpu/drm/i915/i915_scheduler.c
index 6cd7ec4..c3a0680 100644
--- a/drivers/gpu/drm/i915/i915_scheduler.c
+++ b/drivers/gpu/drm/i915/i915_scheduler.c
@@ -25,6 +25,7 @@
 #include "i915_drv.h"
 #include "intel_drv.h"
 #include "i915_scheduler.h"
+#include <../drivers/android/sync.h>
 
 #define for_each_scheduler_node(node, id)				\
 	list_for_each_entry((node), &scheduler->node_queue[(id)], link)
@@ -370,16 +371,96 @@ static inline bool i915_scheduler_is_dependency_valid(
 	return false;
 }
 
+/* Use a private structure in order to pass the 'dev' pointer through */
+struct i915_sync_fence_waiter {
+	struct sync_fence_waiter sfw;
+	struct drm_device	 *dev;
+	struct i915_scheduler_queue_entry *node;
+};
+
+/*
+ * NB: This callback can be executed at interrupt time. Further, it can be
+ * called from within the TDR reset sequence during a scheduler 'kill_all'
+ * and thus be called while the scheduler spinlock is already held. Thus
+ * it can grab neither the driver mutex nor the scheduler spinlock.
+ */
+static void i915_scheduler_wait_fence_signaled(struct sync_fence *fence,
+				       struct sync_fence_waiter *waiter)
+{
+	struct i915_sync_fence_waiter *i915_waiter;
+	struct drm_i915_private *dev_priv = NULL;
+
+	i915_waiter = container_of(waiter, struct i915_sync_fence_waiter, sfw);
+	dev_priv    = (i915_waiter && i915_waiter->dev) ?
+					i915_waiter->dev->dev_private : NULL;
+
+	/*
+	 * NB: The callback is executed at interrupt time, thus it can not
+	 * call _submit() directly. It must go via the delayed work handler.
+	 */
+	if (dev_priv)
+		queue_work(dev_priv->wq, &dev_priv->mm.scheduler_work);
+
+	kfree(waiter);
+}
+
+static bool i915_scheduler_async_fence_wait(struct drm_device *dev,
+				     struct i915_scheduler_queue_entry *node)
+{
+	struct i915_sync_fence_waiter *fence_waiter;
+	struct sync_fence *fence = node->params.fence_wait;
+	int signaled;
+	bool success = true;
+
+	if ((node->flags & I915_QEF_FENCE_WAITING) == 0)
+		node->flags |= I915_QEF_FENCE_WAITING;
+	else
+		return true;
+
+	if (fence == NULL)
+		return false;
+
+	signaled = sync_fence_is_signaled(fence);
+	if (!signaled) {
+		fence_waiter = kmalloc(sizeof(*fence_waiter), GFP_KERNEL);
+		if (!fence_waiter) {
+			success = false;
+			goto end;
+		}
+
+		sync_fence_waiter_init(&fence_waiter->sfw,
+				i915_scheduler_wait_fence_signaled);
+		fence_waiter->node = node;
+		fence_waiter->dev = dev;
+
+		if (sync_fence_wait_async(fence, &fence_waiter->sfw)) {
+			/*
+			 * an error occurred, usually this is because the
+			 * fence was signaled already
+			 */
+			signaled = sync_fence_is_signaled(fence);
+			if (!signaled) {
+				success = false;
+				goto end;
+			}
+		}
+	}
+end:
+	return success;
+}
+
 static int i915_scheduler_pop_from_queue_locked(struct intel_engine_cs *ring,
 				struct i915_scheduler_queue_entry **pop_node)
 {
 	struct drm_i915_private *dev_priv = ring->dev->dev_private;
 	struct i915_scheduler *scheduler = dev_priv->scheduler;
+	struct i915_scheduler_queue_entry *fence_wait = NULL;
+	struct i915_scheduler_queue_entry *best_wait = NULL;
 	struct i915_scheduler_queue_entry *best = NULL;
 	struct i915_scheduler_queue_entry *node;
 	int ret;
 	int i;
-	bool any_queued = false;
+	bool signalled = true, any_queued = false;
 	bool has_local, has_remote, only_remote = false;
 
 	assert_scheduler_lock_held(scheduler);
@@ -392,6 +473,11 @@ static int i915_scheduler_pop_from_queue_locked(struct intel_engine_cs *ring,
 			continue;
 		any_queued = true;
 
+		if (node->params.fence_wait)
+			signalled = sync_fence_is_signaled(node->params.fence_wait);
+		else
+			signalled = true;
+
 		has_local  = false;
 		has_remote = false;
 		for (i = 0; i < node->num_deps; i++) {
@@ -408,9 +494,15 @@ static int i915_scheduler_pop_from_queue_locked(struct intel_engine_cs *ring,
 			only_remote = true;
 
 		if (!has_local && !has_remote) {
-			if (!best ||
-			    (node->priority > best->priority))
-				best = node;
+			if (signalled) {
+				if (!best ||
+				    (node->priority > best->priority))
+					best = node;
+			} else {
+				if (!best_wait ||
+				    (node->priority > best_wait->priority))
+					best_wait = node;
+			}
 		}
 	}
 
@@ -428,8 +520,34 @@ static int i915_scheduler_pop_from_queue_locked(struct intel_engine_cs *ring,
 		 * (a) there are no buffers in the queue
 		 * (b) all queued buffers are dependent on other buffers
 		 *     e.g. on a buffer that is in flight on a different ring
+		 * (c) all independent buffers are waiting on fences
 		 */
-		if (only_remote) {
+		if (best_wait) {
+			/* Need to wait for something to be signalled.
+			 *
+			 * NB: do not really want to wait on one specific fd
+			 * because there is no guarantee in the order that
+			 * blocked buffers will be signalled. Need to wait on
+			 * 'anything' and then rescan for best available, if
+			 * still nothing then wait again...
+			 *
+			 * NB 2: The wait must also wake up if someone attempts
+			 * to submit a new buffer. The new buffer might be
+			 * independent of all others and thus could jump the
+			 * queue and start running immediately.
+			 *
+			 * NB 3: Lastly, must not wait with the spinlock held!
+			 *
+			 * So rather than wait here, need to queue a deferred
+			 * wait thread and just return 'nothing to do'.
+			 *
+			 * NB 4: Can't actually do the wait here because the
+			 * spinlock is still held and the wait requires doing
+			 * a memory allocation.
+			 */
+			fence_wait = best_wait;
+			ret = -EAGAIN;
+		} else if (only_remote) {
 			/* The only dependent buffers are on another ring. */
 			ret = -EAGAIN;
 		} else if (any_queued) {
@@ -439,6 +557,14 @@ static int i915_scheduler_pop_from_queue_locked(struct intel_engine_cs *ring,
 		}
 	}
 
+	if (fence_wait) {
+		/* It should be safe to sleep now... */
+		/* NB: Need to release and reacquire the spinlock though */
+		spin_unlock_irq(&scheduler->lock);
+		i915_scheduler_async_fence_wait(ring->dev, fence_wait);
+		spin_lock_irq(&scheduler->lock);
+	}
+
 	trace_i915_scheduler_pop_from_queue(ring, best);
 
 	*pop_node = best;
@@ -680,6 +806,9 @@ static int i915_scheduler_queue_execbuffer_bypass(struct i915_scheduler_queue_en
 
 	trace_i915_scheduler_queue(qe->params.ring, qe);
 
+	WARN_ON(qe->params.fence_wait &&
+		(!sync_fence_is_signaled(qe->params.fence_wait)));
+
 	intel_ring_reserved_space_cancel(qe->params.request->ringbuf);
 
 	scheduler->flags[qe->params.ring->id] |= I915_SF_SUBMITTING;
@@ -976,6 +1105,11 @@ void i915_scheduler_clean_node(struct i915_scheduler_queue_entry *node)
 	}
 
 	/* And anything else owned by the node: */
+	if (node->params.fence_wait) {
+		sync_fence_put(node->params.fence_wait);
+		node->params.fence_wait = 0;
+	}
+
 	if (node->params.cliprects) {
 		kfree(node->params.cliprects);
 		node->params.cliprects = NULL;
diff --git a/drivers/gpu/drm/i915/i915_scheduler.h b/drivers/gpu/drm/i915/i915_scheduler.h
index 93fb650..edf8459 100644
--- a/drivers/gpu/drm/i915/i915_scheduler.h
+++ b/drivers/gpu/drm/i915/i915_scheduler.h
@@ -55,6 +55,11 @@ struct i915_scheduler_obj_entry {
 	bool read_only;
 };
 
+enum i915_scheduler_queue_entry_flags {
+	/* Fence is being waited on */
+	I915_QEF_FENCE_WAITING              = (1 << 0),
+};
+
 struct i915_scheduler_queue_entry {
 	/* Any information required to submit this batch buffer to the hardware */
 	struct i915_execbuffer_params params;
@@ -74,6 +79,9 @@ struct i915_scheduler_queue_entry {
 	enum i915_scheduler_queue_status status;
 	unsigned long stamp;
 
+	/* See i915_scheduler_queue_entry_flags above */
+	uint32_t flags;
+
 	/* List of all scheduler queue entry nodes */
 	struct list_head link;
 };
-- 
1.7.1

