From 6410387eb0a0b85020a6259819900041952649fe Mon Sep 17 00:00:00 2001
From: Hong Liu <hong.liu@intel.com>
Date: Tue, 9 Aug 2016 15:41:24 +0800
Subject: [PATCH 145/153] [VPG]: drm/i915: add a queue to handle completed nodes

Scheduler may pop up node and add it to the head of the queue before the
scheduler kworker running, so the completed nodes will not sit at the head
of the queue. This will cause completed nodes accumulated on the queue
till the queue is full since currently we only test the head of queue for
completed nodes.

Add a new queue for the completed nodes to avoid iterating through
all nodes to find the completed ones.

Change-Id: I2573b62e8ca8c22efd07ff7b6970f0eb24ae2881
Signed-off-by: Hong Liu <hong.liu@intel.com>
---
 drivers/gpu/drm/i915/i915_scheduler.c |  187 +++++++++++++++++++++-----------
 drivers/gpu/drm/i915/i915_scheduler.h |    1 +
 2 files changed, 124 insertions(+), 64 deletions(-)

diff --git a/drivers/gpu/drm/i915/i915_scheduler.c b/drivers/gpu/drm/i915/i915_scheduler.c
index d8eb8c4..1314cfd 100644
--- a/drivers/gpu/drm/i915/i915_scheduler.c
+++ b/drivers/gpu/drm/i915/i915_scheduler.c
@@ -185,6 +185,7 @@ int i915_scheduler_init(struct drm_device *dev)
 
 	for (r = 0; r < I915_NUM_RINGS; r++)
 		INIT_LIST_HEAD(&scheduler->node_queue[r]);
+	INIT_LIST_HEAD(&scheduler->completed_queue);
 
 	/* Default tuning values: */
 	scheduler->priority_level_min     = -1023;
@@ -245,6 +246,7 @@ static void i915_scheduler_node_kill(struct i915_scheduler *scheduler,
 	}
 
 	node->status = I915_SQS_DEAD;
+	list_move(&node->link, &scheduler->completed_queue);
 	trace_i915_scheduler_node_state_change(node->params.ring, node);
 }
 
@@ -1027,6 +1029,8 @@ bool i915_scheduler_notify_request(struct drm_i915_gem_request *req)
 	trace_i915_scheduler_node_state_change(req->ring, node);
 	scheduler->counts[req->ring->id].flying--;
 
+	list_move(&node->link, &scheduler->completed_queue);
+
 	spin_unlock_irqrestore(&scheduler->lock, flags);
 
 	return true;
@@ -1056,6 +1060,25 @@ static int i915_scheduler_remove_dependent(struct i915_scheduler *scheduler,
 	}
 
 	/*
+	 * Because of the split list, the loop below will no longer remove
+	 * dependencies between completed nodes. Thus you could (briefly)
+	 * end up with a dangling pointer when one completed node is freed
+	 * but still referenced as a dependency by another completed node.
+	 * In practice I don't think there is anywhere that pointer could
+	 * get dereferenced. However, clearing num_deps here should enforce
+	 * that.
+	 *
+	 * Note that the dereference above is safe because all the
+	 * remove_dependent calls are done en masse for the entire current
+	 * set of completed nodes. Then a later step starts freeing them up.
+	 * And a node being processed in this pass cannot have a dangling
+	 * reference to a completed node from a previous pass because at
+	 * that point this node must have been incomplete and would
+	 * therefore have had the dependency stripped.
+	 */
+	remove->num_deps = 0;
+
+	/*
 	 * Remove this node from the dependency lists of any other node which
 	 * might be waiting on it.
 	 */
@@ -1161,19 +1184,17 @@ static bool i915_scheduler_remove(struct i915_scheduler *scheduler,
 				  struct intel_engine_cs *ring,
 				  struct list_head *remove)
 {
-	struct i915_scheduler_queue_entry *node, *node_next;
+	struct i915_scheduler_queue_entry *node;
 	bool do_submit;
 
 	spin_lock_irq(&scheduler->lock);
 
 	INIT_LIST_HEAD(remove);
-	list_for_each_entry_safe(node, node_next, &scheduler->node_queue[ring->id], link) {
-		if (!I915_SQS_IS_COMPLETE(node))
-			break;
 
-		list_del(&node->link);
-		list_add(&node->link, remove);
-		scheduler->stats[ring->id].expired++;
+	list_for_each_entry(node, &scheduler->completed_queue, link) {
+		WARN_ON(!I915_SQS_IS_COMPLETE(node));
+
+		scheduler->stats[node->params.ring->id].expired++;
 
 		/* Strip the dependency info while the mutex is still locked */
 		i915_scheduler_remove_dependent(scheduler, node);
@@ -1183,9 +1204,8 @@ static bool i915_scheduler_remove(struct i915_scheduler *scheduler,
 			i915_scheduler_file_queue_dec(scheduler);
 			node->params.file = NULL;
 		}
-
-		continue;
 	}
+	list_splice_init(&scheduler->completed_queue, remove);
 
 	/*
 	 * Release the interrupt reference count if there are no longer any
@@ -1216,7 +1236,8 @@ static void i915_scheduler_process_work(struct intel_engine_cs *ring)
 	bool do_submit;
 	struct list_head remove;
 
-	if (list_empty(&scheduler->node_queue[ring->id]))
+	if (list_empty(&scheduler->node_queue[ring->id]) &&
+	    list_empty(&scheduler->completed_queue))
 		return;
 
 	/* Remove completed nodes. */
@@ -1238,7 +1259,7 @@ static void i915_scheduler_process_work(struct intel_engine_cs *ring)
 		node = list_first_entry(&remove, typeof(*node), link);
 		list_del(&node->link);
 
-		trace_i915_scheduler_destroy(ring, node);
+		trace_i915_scheduler_destroy(node->params.ring, node);
 
 		/* Free up all the DRM references */
 		i915_scheduler_clean_node(node);
@@ -1307,6 +1328,64 @@ bool i915_scheduler_file_queue_wait(struct drm_file *file)
 #undef COND
 }
 
+static void
+i915_scheduler_dump_node_details(struct i915_scheduler *scheduler,
+				 struct intel_engine_cs *ring,
+				 struct i915_scheduler_queue_entry *node,
+				 uint32_t counts[])
+{
+	int i, deps;
+	uint32_t count;
+
+	if (node->status < I915_SQS_MAX) {
+		count = counts[node->status]++;
+	} else {
+		DRM_DEBUG_DRIVER("<%s>   Unknown status: %d!\n",
+				ring->name, node->status);
+		count = -1;
+	}
+
+	deps = 0;
+	for (i = 0; i < node->num_deps; i++)
+		if (i915_scheduler_is_dependency_valid(node, i))
+			deps++;
+
+	DRM_DEBUG_DRIVER("<%s>   %c:%02d> uniq = %d, seqno"
+			 " = %d/%s, deps = %d / %d, fence = %p/%d, %s [pri = "
+			 "%4d]\n", ring->name,
+			 i915_scheduler_queue_status_chr(node->status),
+			 count,
+			 node->params.request->uniq,
+			 node->params.request->seqno,
+			 node->params.ring->name,
+			 deps, node->num_deps,
+			 node->params.fence_wait,
+			 node->params.fence_wait ? sync_fence_is_signaled(node->params.fence_wait) : 0,
+			 i915_qe_state_str(node),
+			 node->priority);
+
+	if ((scheduler->flags[ring->id] & I915_SF_DUMP_DEPENDENCIES) == 0)
+		return;
+
+	for (i = 0; i < node->num_deps; i++) {
+		if (!node->dep_list[i])
+			continue;
+
+		DRM_DEBUG_DRIVER("<%s>       |-%c:"
+				 "%02d%c uniq = %d, seqno = %d/%s, %s [pri = %4d]\n",
+				 ring->name,
+				 i915_scheduler_queue_status_chr(node->dep_list[i]->status),
+				 i,
+				 i915_scheduler_is_dependency_valid(node, i)
+				 ? '>' : '#',
+				 node->dep_list[i]->params.request->uniq,
+				 node->dep_list[i]->params.request->seqno,
+				 node->dep_list[i]->params.ring->name,
+				 i915_qe_state_str(node->dep_list[i]),
+				 node->dep_list[i]->priority);
+	}
+}
+
 static int i915_scheduler_dump_locked(struct intel_engine_cs *ring,
 				      const char *msg)
 {
@@ -1326,12 +1405,15 @@ static int i915_scheduler_dump_locked(struct intel_engine_cs *ring,
 			queued++;
 		else if (I915_SQS_IS_FLYING(node))
 			flying++;
-		else if (I915_SQS_IS_COMPLETE(node))
-			complete++;
 		else
 			other++;
 	}
 
+	list_for_each_entry(node, &scheduler->completed_queue, link) {
+		if (node->params.ring == ring)
+			complete++;
+	}
+
 	b_dump = (flying != old_flying) ||
 		 (queued != old_queued) ||
 		 (complete != old_complete);
@@ -1375,57 +1457,20 @@ static int i915_scheduler_dump_locked(struct intel_engine_cs *ring,
 	}
 
 	if (scheduler->flags[ring->id] & I915_SF_DUMP_DETAILS) {
-		int i, deps;
-		uint32_t count, counts[I915_SQS_MAX];
+		uint32_t counts[I915_SQS_MAX];
 
 		memset(counts, 0x00, sizeof(counts));
 
 		for_each_scheduler_node(node, ring->id) {
-			if (node->status < I915_SQS_MAX) {
-				count = counts[node->status]++;
-			} else {
-				DRM_DEBUG_DRIVER("<%s>   Unknown status: %d!\n",
-						 ring->name, node->status);
-				count = -1;
-			}
-
-			deps = 0;
-			for (i = 0; i < node->num_deps; i++)
-				if (i915_scheduler_is_dependency_valid(node, i))
-					deps++;
-
-			DRM_DEBUG_DRIVER("<%s>   %c:%02d> uniq = %d, seqno"
-					 " = %d/%s, deps = %d / %d, fence = %p/%d, %s [pri = "
-					 "%4d]\n", ring->name,
-					 i915_scheduler_queue_status_chr(node->status),
-					 count,
-					 node->params.request->uniq,
-					 node->params.request->seqno,
-					 node->params.ring->name,
-					 deps, node->num_deps,
-					 node->params.fence_wait,
-					 node->params.fence_wait ? sync_fence_is_signaled(node->params.fence_wait) : 0,
-					 i915_qe_state_str(node),
-					 node->priority);
+			i915_scheduler_dump_node_details(scheduler,
+							 ring, node, counts);
+		}
 
-			if ((scheduler->flags[ring->id] & I915_SF_DUMP_DEPENDENCIES)
-				== 0)
+		list_for_each_entry(node, &scheduler->completed_queue, link) {
+			if (node->params.ring != ring)
 				continue;
-
-			for (i = 0; i < node->num_deps; i++)
-				if (node->dep_list[i])
-					DRM_DEBUG_DRIVER("<%s>       |-%c:"
-						"%02d%c uniq = %d, seqno = %d/%s, %s [pri = %4d]\n",
-						ring->name,
-						i915_scheduler_queue_status_chr(node->dep_list[i]->status),
-						i,
-						i915_scheduler_is_dependency_valid(node, i)
-							? '>' : '#',
-						node->dep_list[i]->params.request->uniq,
-						node->dep_list[i]->params.request->seqno,
-						node->dep_list[i]->params.ring->name,
-						i915_qe_state_str(node->dep_list[i]),
-						node->dep_list[i]->priority);
+			i915_scheduler_dump_node_details(scheduler,
+							 ring, node, counts);
 		}
 	}
 
@@ -1526,6 +1571,21 @@ int i915_scheduler_query_stats(struct intel_engine_cs *ring,
 		stats->counts[node->status]++;
 	}
 
+	list_for_each_entry(node, &scheduler->completed_queue, link) {
+		if (node->params.ring != ring)
+			continue;
+
+		if (node->status >= I915_SQS_MAX) {
+			DRM_DEBUG_DRIVER("Invalid node state: %d! [uniq = %d, seqno = %d]\n",
+					 node->status, node->params.request->uniq,
+					 node->params.request->seqno);
+
+			stats->counts[I915_SQS_MAX]++;
+			continue;
+		}
+		stats->counts[node->status]++;
+	}
+
 	if (stats->counts[I915_SQS_QUEUED] != scheduler->counts[ring->id].queued)
 		printk(KERN_ERR "%s:%d> \x1B[31;1mQueued count mis-match: %d vs %d!\x1B[0m\n", __func__, __LINE__, stats->counts[I915_SQS_QUEUED], scheduler->counts[ring->id].queued);
 	if (stats->counts[I915_SQS_FLYING] != scheduler->counts[ring->id].flying)
@@ -1831,12 +1891,11 @@ void i915_scheduler_closefile(struct drm_device *dev, struct drm_file *file)
 			if (node->params.file != file)
 				continue;
 
-			if (!I915_SQS_IS_COMPLETE(node))
-				DRM_DEBUG_DRIVER("Closing file handle with outstanding work: %d:%d/%s on %s\n",
-						 node->params.request->uniq,
-						 node->params.request->seqno,
-						 i915_qe_state_str(node),
-						 ring->name);
+			DRM_DEBUG_DRIVER("Closing file handle with outstanding work: %d:%d/%s on %s\n",
+					 node->params.request->uniq,
+					 node->params.request->seqno,
+					 i915_qe_state_str(node),
+					 ring->name);
 
 			i915_scheduler_file_queue_dec(scheduler);
 			node->params.file = NULL;
diff --git a/drivers/gpu/drm/i915/i915_scheduler.h b/drivers/gpu/drm/i915/i915_scheduler.h
index ca5867d..9c5a309 100644
--- a/drivers/gpu/drm/i915/i915_scheduler.h
+++ b/drivers/gpu/drm/i915/i915_scheduler.h
@@ -132,6 +132,7 @@ struct i915_scheduler_node_states {
 
 struct i915_scheduler {
 	struct list_head node_queue[I915_NUM_RINGS];
+	struct list_head completed_queue;
 	uint32_t flags[I915_NUM_RINGS];
 	spinlock_t lock;
 
-- 
1.7.1

